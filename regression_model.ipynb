{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab0d39",
   "metadata": {},
   "source": [
    "#Schedule for project:\n",
    "# 1. Introduction and setup and numpy/python basics\n",
    "# 2. Data loading and exploration\n",
    "# 3. Data preprocessing and feature engineering\n",
    "# 4. Model 1 training\n",
    "# 5. Model 2 training?\n",
    "# 6. Model evaluation and visualization\n",
    "# 7. Hyperparameter tuning and model selection\n",
    "# 8. Wrap-up and make a website!\n",
    "\n",
    "#TO DO:\n",
    "# 1. Add a function to load data from a CSV file\n",
    "# 2. Implement a function to preprocess the data\n",
    "# 3. Create a function to train logistic regression model\n",
    "# 4. Create a function to train a kernel ridge regression or svm model\n",
    "# 4. Implement a function to evaluate the model\n",
    "# 5. Add a function to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train logistic regression model\n",
    "def train_model(X: npt.NDArray, y: npt.NDArray):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    X (npt.NDArray): Feature matrix.\n",
    "    y (npt.NDArray): Target vector.\n",
    "    \n",
    "    Returns:\n",
    "    model: Trained log regrression model.\n",
    "    \"\"\"\n",
    "    #May need to use k-stratified cross-validation depending on the dataset\n",
    "    # Initialize the logistic regression model\n",
    "    model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "    \n",
    "    # Could use RFE for feature selection\n",
    "    #rfe = RFE(estimator=model, n_features_to_select=10)\n",
    "    #rfe.fit(X, y)\n",
    "    \n",
    "    # Fit the model with selected features\n",
    "    model.fit(X[:, model.support_], y)\n",
    "\n",
    "    #Can use model.coef_ or model.intercept_ for students to explore/visualize the regression model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b56426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate the trained model\n",
    "def evaluate_model(model, X: npt.NDArray, y_true: npt.NDArray):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    model: Trained logistic regression model.\n",
    "    X (npt.NDArray): Feature matrix.\n",
    "    y (npt.NDArray): Target vector.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Evaluation metrics.\n",
    "    \"\"\"\n",
    "    #Predict probabilities\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    #Calculate AUROC score\n",
    "    auroc = metrics.roc_auc_score(y_true, y_pred, labels = [-1,1])\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "    #Calculate precision\n",
    "    precision = metrics.precision_score(y_true, y_pred, zero_division = 0.0)\n",
    "    \n",
    "    #Calculate recall\n",
    "    recall = metrics.recall_score(y_true,y_pred)\n",
    "\n",
    "    #Calculate F1 score\n",
    "    f1_score = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "    #Calculate specificity\n",
    "    conf_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    denom = conf_matrix[0][0]+conf_matrix[0][1]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    specificity = conf_matrix[0][0]/(denom)\n",
    "    \n",
    "    #Calculate average precision\n",
    "    average_precision = metrics.average_precision_score(y_true, y_pred)\n",
    "   \n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1_score, 'specificity': specificity, \n",
    "            'average_precision': average_precision, 'auroc': auroc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d391ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(y_true: npt.NDArray, y_pred: npt.NDArray):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    y_true (npt.NDArray): True labels.\n",
    "    y_pred (npt.NDArray): Predicted labels.\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays the ROC curve and confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot ROC curve\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "\n",
    "    \n",
    "    #fpr, tpr, _ = roc_curve(test_df['True'], test_df[model])\n",
    "    #roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{model} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Random Guess')\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Plot confusion matrix"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
